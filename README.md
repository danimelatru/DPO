# Post-Training Alignment for Agentic AI using DPO

A project focused on **post-training alignment** of Small Language Models (SLMs) for **agentic tool use**. This repository implements an end-to-end pipeline to steer models towards strict "Chain-of-Thought" reasoning and structured JSON actions using **Direct Preference Optimization (DPO)**.

> **Research Goal:** Validate the efficacy of DPO in enforcing rigid schema compliance (JSON) and reasoning traces in lightweight models (1.1Bâ€“8B parameters) without relying on massive-scale supervised fine-tuning.

---

## ğŸš€ Key Features

- **Synthetic Agentic Dataset:** Automated generation of "Thought â†’ Action" trajectories simulating tool use (Calculator, Search, Calendar).
- **DPO Fine-Tuning:** Implementation of Direct Preference Optimization to penalize hallucinations and unstructured outputs.
- **HPC Optimized:** Full SLURM integration for training on A100 clusters using mixed-precision (`bf16`).
- **Parameter Efficient:** Uses LoRA (Low-Rank Adaptation) techniques for efficient fine-tuning.
- **Interactive CLI:** Real-time testing interface for conversational agent interaction.

---

## ğŸ“‚ Project Structure

```text
dpo/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ train_config.yaml
â”‚   â””â”€â”€ train_config_tinyllama.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ logs/
â”œâ”€â”€ models/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_training.slurm
â”‚   â”œâ”€â”€ run_inference.slurm
â”‚   â””â”€â”€ run_interactive.slurm
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generate_data.py
â”‚   â”‚   â””â”€â”€ data_loader.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train_dpo.py
â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ inference_llama3.py
â”‚   â”‚   â””â”€â”€ interactive_cli.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ prompts.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Installation

### Requirements
- Python 3.10+
- CUDA 11.8+
- 16GB+ GPU memory (24GB+ recommended for Llama 3)

### Setup

```bash
git clone https://github.com/danimelatru/dpo.git
cd dpo

conda create -n dpo python=3.10 -y
conda activate dpo

pip install -r requirements.txt

mkdir -p logs models data/processed
```

---

## ğŸ§ª Usage

### 1. Generate Synthetic Training Data

```bash
python -m src.data.generate_data --output data/processed/dpo_data.jsonl --num-examples 2500 --seed 42
```

### 2. Training

```bash
python -m src.training.train_dpo --config configs/train_config_tinyllama.yaml
```

or via SLURM:

```bash
sbatch scripts/run_training.slurm
```

---

## ğŸ“Š Results

| Metric | Base Model | After DPO |
|------|-----------|-----------|
| JSON Validity | 0% | 95%+ |
| Thought Prefix Rate | 10% | 98%+ |
| Full Compliance | 0% | 93%+ |

---

## ğŸ“œ License

This project is open-sourced under the MIT License.
